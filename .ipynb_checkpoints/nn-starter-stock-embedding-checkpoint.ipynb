{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-05T17:50:17.732318Z",
     "iopub.status.busy": "2021-07-05T17:50:17.731903Z",
     "iopub.status.idle": "2021-07-05T17:50:19.647572Z",
     "shell.execute_reply": "2021-07-05T17:50:19.646491Z",
     "shell.execute_reply.started": "2021-07-05T17:50:17.732235Z"
    }
   },
   "source": [
    "## NN starter\n",
    "\n",
    "From https://www.kaggle.com/lucasmorin/tf-keras-nn-with-stock-embedding\n",
    "\n",
    "A simple NN starter using stock Embedding. \n",
    "\n",
    "Heavily inspired from this notebook for the feature engineering part:\n",
    "https://www.kaggle.com/manels/lgb-starter\n",
    "\n",
    "Embedding layer from :\n",
    "https://www.kaggle.com/colinmorris/embedding-layers\n",
    "\n",
    "Also see:\n",
    "* https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data\n",
    "* https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/250324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T06:59:10.267034Z",
     "iopub.status.busy": "2021-08-07T06:59:10.266621Z",
     "iopub.status.idle": "2021-08-07T06:59:11.379745Z",
     "shell.execute_reply": "2021-08-07T06:59:11.378596Z",
     "shell.execute_reply.started": "2021-08-07T06:59:10.266946Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = 'data'\n",
    "path_data = 'data'\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T07:11:53.353638Z",
     "iopub.status.busy": "2021-08-07T07:11:53.353252Z",
     "iopub.status.idle": "2021-08-07T07:11:53.371545Z",
     "shell.execute_reply": "2021-08-07T07:11:53.370694Z",
     "shell.execute_reply.started": "2021-08-07T07:11:53.353607Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff() \n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def get_stock_stat(stock_id : int, dataType = 'train'):\n",
    "    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n",
    "    \n",
    "    #Book features\n",
    "    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet/stock_id={}/'.format(dataType, stock_id)))\n",
    "    df_book['stock_id'] = stock_id\n",
    "    cols = key + [col for col in df_book.columns if col not in key]\n",
    "    df_book = df_book[cols]\n",
    "    \n",
    "    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n",
    "                                    df_book['ask_price1'] * df_book['bid_size1']) / (df_book['bid_size1'] + df_book['ask_size1'])\n",
    "    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n",
    "                                    df_book['ask_price2'] * df_book['bid_size2']) / (df_book['bid_size2'] + df_book['ask_size2'])\n",
    "    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n",
    "    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n",
    "    \n",
    "    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n",
    "    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n",
    "                        .agg(realized_volatility).reset_index()\n",
    "\n",
    "    #Trade features\n",
    "    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet/stock_id={}'.format(dataType, stock_id)))\n",
    "    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n",
    "    trade_stat['stock_id'] = stock_id\n",
    "    cols = key + [col for col in trade_stat.columns if col not in key]\n",
    "    trade_stat = trade_stat[cols]\n",
    "    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n",
    "    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n",
    "                           .agg(realized_volatility).reset_index()\n",
    "    #Joining book and trade features\n",
    "    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n",
    "    \n",
    "    return stock_stat\n",
    "\n",
    "def get_dataSet(stock_ids : list, dataType = 'train'):\n",
    "\n",
    "    stock_stat = Parallel(n_jobs=-1)(\n",
    "        delayed(get_stock_stat)(stock_id, dataType) \n",
    "        for stock_id in stock_ids\n",
    "    )\n",
    "    \n",
    "    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n",
    "\n",
    "    return stock_stat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T07:11:59.561715Z",
     "iopub.status.busy": "2021-08-07T07:11:59.561123Z",
     "iopub.status.idle": "2021-08-07T07:20:01.315329Z",
     "shell.execute_reply": "2021-08-07T07:20:01.314153Z",
     "shell.execute_reply.started": "2021-08-07T07:11:59.561663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yzho0040\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n",
      "Train shape: (428932, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>log_return1</th>\n",
       "      <th>log_return2</th>\n",
       "      <th>trade_log_return1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.002006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target  log_return1  log_return2  trade_log_return1\n",
       "0         0        5  0.004136     0.004499     0.006999           0.002006\n",
       "1         0       11  0.001445     0.001204     0.002476           0.000901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (3, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>log_return1</th>\n",
       "      <th>log_return2</th>\n",
       "      <th>trade_log_return1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  log_return1  log_return2  trade_log_return1\n",
       "0         0        4    0-4     0.000294     0.000252           0.000295\n",
       "1         0       32   0-32     0.000000     0.000000           0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n",
    "%time train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\n",
    "train = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\n",
    "print('Train shape: {}'.format(train.shape))\n",
    "display(train.head(2))\n",
    "\n",
    "test = pd.read_csv(os.path.join(path_data, 'test.csv'))\n",
    "test_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\n",
    "test = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\n",
    "print('Test shape: {}'.format(test.shape))\n",
    "display(test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-10T14:05:25.007244Z",
     "iopub.status.busy": "2021-07-10T14:05:25.006895Z",
     "iopub.status.idle": "2021-07-10T14:05:25.012276Z",
     "shell.execute_reply": "2021-07-10T14:05:25.011158Z",
     "shell.execute_reply.started": "2021-07-10T14:05:25.007208Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-10T14:48:10.458406Z",
     "iopub.status.busy": "2021-07-10T14:48:10.457993Z",
     "iopub.status.idle": "2021-07-10T14:48:10.468387Z",
     "shell.execute_reply": "2021-07-10T14:48:10.467498Z",
     "shell.execute_reply.started": "2021-07-10T14:48:10.45835Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_units = (32,16,8,4,2)\n",
    "stock_embedding_size = 16\n",
    "\n",
    "cat_data = train['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(3,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='selu')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-10T14:48:11.211662Z",
     "iopub.status.busy": "2021-07-10T14:48:11.211132Z",
     "iopub.status.idle": "2021-07-10T14:48:11.21737Z",
     "shell.execute_reply": "2021-07-10T14:48:11.216219Z",
     "shell.execute_reply.started": "2021-07-10T14:48:11.211632Z"
    }
   },
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-05, patience=10, verbose=1,\n",
    "    mode='min', baseline=0.25)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=3, verbose=1,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-10T14:48:11.996796Z",
     "iopub.status.busy": "2021-07-10T14:48:11.996404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/4\n",
      "Epoch 1/100\n",
      "285/315 [==========================>...] - ETA: 0s - loss: 263.9146 - MSE: 3.9780e-04WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "315/315 [==============================] - 7s 22ms/step - loss: 239.4375 - MSE: 3.6163e-04 - val_loss: 0.2321 - val_MSE: 7.7009e-06\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 1.0037 - MSE: 9.9820e-06 - val_loss: 0.1468 - val_MSE: 5.0894e-06\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1123 - MSE: 4.2563e-06 - val_loss: 0.0982 - val_MSE: 2.9434e-06\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0859 - MSE: 2.9066e-06 - val_loss: 0.1956 - val_MSE: 2.5227e-06\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.3771 - MSE: 3.9786e-06 - val_loss: 0.0721 - val_MSE: 2.3475e-06\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0840 - MSE: 2.4322e-06 - val_loss: 0.0827 - val_MSE: 2.1054e-06\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1037 - MSE: 2.4806e-06 - val_loss: 0.0760 - val_MSE: 2.1716e-06\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1419 - MSE: 2.6929e-06 - val_loss: 0.0707 - val_MSE: 2.2956e-06\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1977 - MSE: 2.9395e-06 - val_loss: 1.4619 - val_MSE: 1.2006e-05\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.2422 - MSE: 3.2636e-06 - val_loss: 0.0776 - val_MSE: 2.3247e-06\n",
      "Epoch 11/100\n",
      "290/315 [==========================>...] - ETA: 0s - loss: 7.4776 - MSE: 4.5009e-05\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 6.9190 - MSE: 4.2191e-05 - val_loss: 0.3151 - val_MSE: 9.6094e-06\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1923 - MSE: 8.4189e-06 - val_loss: 0.1892 - val_MSE: 7.5806e-06\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1659 - MSE: 7.5287e-06 - val_loss: 0.1544 - val_MSE: 6.8474e-06\n",
      "Epoch 14/100\n",
      "288/315 [==========================>...] - ETA: 0s - loss: 0.1443 - MSE: 6.4871e-06\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1430 - MSE: 6.4363e-06 - val_loss: 0.1289 - val_MSE: 5.5020e-06\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1244 - MSE: 5.6449e-06 - val_loss: 0.1211 - val_MSE: 5.4489e-06\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1178 - MSE: 5.3247e-06 - val_loss: 0.1156 - val_MSE: 5.1321e-06\n",
      "Epoch 17/100\n",
      "290/315 [==========================>...] - ETA: 0s - loss: 0.1114 - MSE: 4.9936e-06\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1110 - MSE: 4.9622e-06 - val_loss: 0.1068 - val_MSE: 4.7176e-06\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1059 - MSE: 4.7318e-06 - val_loss: 0.1051 - val_MSE: 4.5851e-06\n",
      "Epoch 00018: early stopping\n",
      "Fold 1 NN: 0.32425\n",
      "CV 2/4\n",
      "Epoch 1/100\n",
      "315/315 [==============================] - 7s 23ms/step - loss: 614.2534 - MSE: 0.0025 - val_loss: 74.9297 - val_MSE: 6.6536e-05\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 69.0623 - MSE: 4.3842e-05 - val_loss: 32.4150 - val_MSE: 3.0009e-05\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 29.4913 - MSE: 2.0825e-05 - val_loss: 10.9329 - val_MSE: 1.2585e-05\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 9.5139 - MSE: 9.3028e-06 - val_loss: 2.2547 - val_MSE: 5.3352e-06\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 1.9061 - MSE: 3.7641e-06 - val_loss: 0.3501 - val_MSE: 3.0375e-06\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.2873 - MSE: 2.9010e-06 - val_loss: 0.1593 - val_MSE: 2.9189e-06\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0946 - MSE: 2.5575e-06 - val_loss: 0.0789 - val_MSE: 2.6740e-06\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0905 - MSE: 2.5070e-06 - val_loss: 0.0771 - val_MSE: 2.4916e-06\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1168 - MSE: 2.6173e-06 - val_loss: 0.0950 - val_MSE: 2.3895e-06\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1073 - MSE: 2.5557e-06 - val_loss: 0.0834 - val_MSE: 2.6335e-06\n",
      "Epoch 11/100\n",
      "272/315 [========================>.....] - ETA: 0s - loss: 0.1061 - MSE: 2.5429e-06\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1078 - MSE: 2.5441e-06 - val_loss: 0.1399 - val_MSE: 3.1031e-06\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0735 - MSE: 2.3666e-06 - val_loss: 0.0729 - val_MSE: 2.4407e-06\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0719 - MSE: 2.3531e-06 - val_loss: 0.0764 - val_MSE: 2.3852e-06\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0752 - MSE: 2.3600e-06 - val_loss: 0.0750 - val_MSE: 2.2289e-06\n",
      "Epoch 15/100\n",
      "311/315 [============================>.] - ETA: 0s - loss: 0.0779 - MSE: 2.3630e-06\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0778 - MSE: 2.3588e-06 - val_loss: 0.0740 - val_MSE: 2.4757e-06\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0690 - MSE: 2.3156e-06 - val_loss: 0.0708 - val_MSE: 2.3131e-06\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0691 - MSE: 2.3140e-06 - val_loss: 0.0782 - val_MSE: 2.5681e-06\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0690 - MSE: 2.3143e-06 - val_loss: 0.0709 - val_MSE: 2.3676e-06\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0706 - MSE: 2.3193e-06 - val_loss: 0.0701 - val_MSE: 2.3922e-06\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0701 - MSE: 2.3183e-06 - val_loss: 0.0711 - val_MSE: 2.3021e-06\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0700 - MSE: 2.3122e-06 - val_loss: 0.0848 - val_MSE: 2.6916e-06\n",
      "Epoch 22/100\n",
      "287/315 [==========================>...] - ETA: 0s - loss: 0.0738 - MSE: 2.3403e-06\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0734 - MSE: 2.3313e-06 - val_loss: 0.0710 - val_MSE: 2.2806e-06\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0681 - MSE: 2.2978e-06 - val_loss: 0.0703 - val_MSE: 2.3013e-06\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0687 - MSE: 2.3039e-06 - val_loss: 0.0698 - val_MSE: 2.3276e-06\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0685 - MSE: 2.3034e-06 - val_loss: 0.0699 - val_MSE: 2.2976e-06\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0690 - MSE: 2.3040e-06 - val_loss: 0.0695 - val_MSE: 2.3678e-06\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0688 - MSE: 2.3028e-06 - val_loss: 0.0705 - val_MSE: 2.2565e-06\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0688 - MSE: 2.3008e-06 - val_loss: 0.0702 - val_MSE: 2.3298e-06\n",
      "Epoch 29/100\n",
      "284/315 [==========================>...] - ETA: 0s - loss: 0.0691 - MSE: 2.2930e-06\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0693 - MSE: 2.3003e-06 - val_loss: 0.0708 - val_MSE: 2.3933e-06\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0677 - MSE: 2.2934e-06 - val_loss: 0.0692 - val_MSE: 2.3577e-06\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0676 - MSE: 2.2933e-06 - val_loss: 0.0690 - val_MSE: 2.3502e-06\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0675 - MSE: 2.2930e-06 - val_loss: 0.0697 - val_MSE: 2.3175e-06\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0679 - MSE: 2.2928e-06 - val_loss: 0.0692 - val_MSE: 2.3412e-06\n",
      "Epoch 34/100\n",
      "280/315 [=========================>....] - ETA: 0s - loss: 0.0677 - MSE: 2.2963e-06\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0680 - MSE: 2.2933e-06 - val_loss: 0.0691 - val_MSE: 2.3177e-06\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0673 - MSE: 2.2921e-06 - val_loss: 0.0690 - val_MSE: 2.3139e-06\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0672 - MSE: 2.2878e-06 - val_loss: 0.0689 - val_MSE: 2.3440e-06\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0673 - MSE: 2.2892e-06 - val_loss: 0.0690 - val_MSE: 2.3134e-06\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0673 - MSE: 2.2854e-06 - val_loss: 0.0715 - val_MSE: 2.4573e-06\n",
      "Epoch 39/100\n",
      "280/315 [=========================>....] - ETA: 0s - loss: 0.0675 - MSE: 2.2852e-06\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0675 - MSE: 2.2919e-06 - val_loss: 0.0688 - val_MSE: 2.3333e-06\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0671 - MSE: 2.2882e-06 - val_loss: 0.0689 - val_MSE: 2.3302e-06\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0671 - MSE: 2.2879e-06 - val_loss: 0.0689 - val_MSE: 2.3266e-06\n",
      "Epoch 42/100\n",
      "285/315 [==========================>...] - ETA: 0s - loss: 0.0673 - MSE: 2.2884e-06\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0671 - MSE: 2.2881e-06 - val_loss: 0.0689 - val_MSE: 2.3211e-06\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 0.0671 - MSE: 2.2865e-06 - val_loss: 0.0689 - val_MSE: 2.3235e-06\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0671 - MSE: 2.2865e-06 - val_loss: 0.0689 - val_MSE: 2.3271e-06\n",
      "Epoch 45/100\n",
      "284/315 [==========================>...] - ETA: 0s - loss: 0.0671 - MSE: 2.2911e-06\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0671 - MSE: 2.2879e-06 - val_loss: 0.0689 - val_MSE: 2.3274e-06\n",
      "Epoch 46/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2876e-06 - val_loss: 0.0689 - val_MSE: 2.3283e-06\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2884e-06 - val_loss: 0.0689 - val_MSE: 2.3279e-06\n",
      "Epoch 48/100\n",
      "285/315 [==========================>...] - ETA: 0s - loss: 0.0667 - MSE: 2.2875e-06\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2878e-06 - val_loss: 0.0689 - val_MSE: 2.3280e-06\n",
      "Epoch 49/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2880e-06 - val_loss: 0.0689 - val_MSE: 2.3279e-06\n",
      "Epoch 00049: early stopping\n",
      "Fold 2 NN: 0.26247\n",
      "CV 3/4\n",
      "Epoch 1/100\n",
      "315/315 [==============================] - 7s 22ms/step - loss: 3236.9514 - MSE: 0.0012 - val_loss: 3.5717 - val_MSE: 2.3128e-05\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 2.4189 - MSE: 4.9688e-06 - val_loss: 0.1193 - val_MSE: 2.4985e-06\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1611 - MSE: 2.8583e-06 - val_loss: 0.1221 - val_MSE: 3.1276e-06\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1802 - MSE: 2.9518e-06 - val_loss: 0.4941 - val_MSE: 5.4973e-06\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.3659 - MSE: 3.7785e-06 - val_loss: 0.0759 - val_MSE: 2.5742e-06\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1668 - MSE: 2.8425e-06 - val_loss: 0.0955 - val_MSE: 2.5344e-06\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.6220 - MSE: 4.9767e-06 - val_loss: 0.1650 - val_MSE: 3.2219e-06\n",
      "Epoch 8/100\n",
      "282/315 [=========================>....] - ETA: 0s - loss: 5.6171 - MSE: 1.2982e-05\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 5.0880 - MSE: 1.2292e-05 - val_loss: 1.3677 - val_MSE: 1.0642e-05\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 2.3239 - MSE: 4.0228e-06 - val_loss: 0.2305 - val_MSE: 3.4376e-06\n",
      "Epoch 10/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.8689 - MSE: 3.0712e-06 - val_loss: 0.0750 - val_MSE: 2.6402e-06\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0989 - MSE: 2.5023e-06 - val_loss: 0.0699 - val_MSE: 2.4626e-06\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0793 - MSE: 2.4328e-06 - val_loss: 0.0721 - val_MSE: 2.2109e-06\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0809 - MSE: 2.3790e-06 - val_loss: 0.0704 - val_MSE: 2.3474e-06\n",
      "Epoch 14/100\n",
      "280/315 [=========================>....] - ETA: 0s - loss: 0.0795 - MSE: 2.3615e-06\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "315/315 [==============================] - 0s 2ms/step - loss: 0.0795 - MSE: 2.3459e-06 - val_loss: 0.0703 - val_MSE: 2.2913e-06\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0693 - MSE: 2.2847e-06 - val_loss: 0.0668 - val_MSE: 2.2569e-06\n",
      "Epoch 16/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0705 - MSE: 2.2798e-06 - val_loss: 0.0670 - val_MSE: 2.2779e-06\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0701 - MSE: 2.2751e-06 - val_loss: 0.0679 - val_MSE: 2.1662e-06\n",
      "Epoch 18/100\n",
      "274/315 [=========================>....] - ETA: 0s - loss: 0.0703 - MSE: 2.2698e-06\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0706 - MSE: 2.2689e-06 - val_loss: 0.0681 - val_MSE: 2.3052e-06\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0678 - MSE: 2.2524e-06 - val_loss: 0.0666 - val_MSE: 2.2647e-06\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0680 - MSE: 2.2518e-06 - val_loss: 0.0670 - val_MSE: 2.2863e-06\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0684 - MSE: 2.2520e-06 - val_loss: 0.0664 - val_MSE: 2.2500e-06\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0683 - MSE: 2.2522e-06 - val_loss: 0.0663 - val_MSE: 2.2179e-06\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0678 - MSE: 2.2469e-06 - val_loss: 0.0836 - val_MSE: 2.5622e-06\n",
      "Epoch 24/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0690 - MSE: 2.2536e-06 - val_loss: 0.0709 - val_MSE: 2.1052e-06\n",
      "Epoch 25/100\n",
      "272/315 [========================>.....] - ETA: 0s - loss: 0.0687 - MSE: 2.2605e-06\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0683 - MSE: 2.2496e-06 - val_loss: 0.0667 - val_MSE: 2.2461e-06\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0674 - MSE: 2.2385e-06 - val_loss: 0.0679 - val_MSE: 2.1375e-06\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0674 - MSE: 2.2387e-06 - val_loss: 0.0663 - val_MSE: 2.2420e-06\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/315 [=========================>....] - ETA: 0s - loss: 0.0672 - MSE: 2.2203e-06\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0672 - MSE: 2.2413e-06 - val_loss: 0.0665 - val_MSE: 2.1745e-06\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2356e-06 - val_loss: 0.0661 - val_MSE: 2.1982e-06\n",
      "Epoch 30/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2363e-06 - val_loss: 0.0660 - val_MSE: 2.2175e-06\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2377e-06 - val_loss: 0.0662 - val_MSE: 2.1832e-06\n",
      "Epoch 32/100\n",
      "273/315 [=========================>....] - ETA: 0s - loss: 0.0671 - MSE: 2.2226e-06\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.2371e-06 - val_loss: 0.0660 - val_MSE: 2.2121e-06\n",
      "Epoch 33/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2365e-06 - val_loss: 0.0660 - val_MSE: 2.2051e-06\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2358e-06 - val_loss: 0.0660 - val_MSE: 2.2027e-06\n",
      "Epoch 35/100\n",
      "306/315 [============================>.] - ETA: 0s - loss: 0.0669 - MSE: 2.2410e-06\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2352e-06 - val_loss: 0.0660 - val_MSE: 2.2102e-06\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2368e-06 - val_loss: 0.0660 - val_MSE: 2.2079e-06\n",
      "Epoch 37/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2357e-06 - val_loss: 0.0660 - val_MSE: 2.2061e-06\n",
      "Epoch 38/100\n",
      "271/315 [========================>.....] - ETA: 0s - loss: 0.0668 - MSE: 2.2350e-06\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2354e-06 - val_loss: 0.0660 - val_MSE: 2.2121e-06\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2391e-06 - val_loss: 0.0660 - val_MSE: 2.2090e-06\n",
      "Epoch 40/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2368e-06 - val_loss: 0.0660 - val_MSE: 2.2069e-06\n",
      "Epoch 41/100\n",
      "276/315 [=========================>....] - ETA: 0s - loss: 0.0670 - MSE: 2.2329e-06\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2360e-06 - val_loss: 0.0660 - val_MSE: 2.2074e-06\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2358e-06 - val_loss: 0.0660 - val_MSE: 2.2079e-06\n",
      "Epoch 43/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2359e-06 - val_loss: 0.0660 - val_MSE: 2.2080e-06\n",
      "Epoch 44/100\n",
      "278/315 [=========================>....] - ETA: 0s - loss: 0.0668 - MSE: 2.2352e-06\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.023999907090456e-09.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2362e-06 - val_loss: 0.0660 - val_MSE: 2.2079e-06\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.2362e-06 - val_loss: 0.0660 - val_MSE: 2.2079e-06\n",
      "Epoch 00045: early stopping\n",
      "Fold 3 NN: 0.2569\n",
      "CV 4/4\n",
      "Epoch 1/100\n",
      "315/315 [==============================] - 7s 21ms/step - loss: 77.3210 - MSE: 3.8903e-04 - val_loss: 0.2250 - val_MSE: 9.5630e-06\n",
      "Epoch 2/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.2343 - MSE: 7.1970e-06 - val_loss: 0.1273 - val_MSE: 5.2307e-06\n",
      "Epoch 3/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1112 - MSE: 4.0478e-06 - val_loss: 0.1017 - val_MSE: 3.5873e-06\n",
      "Epoch 4/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1038 - MSE: 2.8270e-06 - val_loss: 0.0783 - val_MSE: 2.6585e-06\n",
      "Epoch 5/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0865 - MSE: 2.5255e-06 - val_loss: 0.0845 - val_MSE: 2.7773e-06\n",
      "Epoch 6/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1079 - MSE: 2.5647e-06 - val_loss: 0.0722 - val_MSE: 2.4219e-06\n",
      "Epoch 7/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.2026 - MSE: 2.9950e-06 - val_loss: 0.0705 - val_MSE: 2.1455e-06\n",
      "Epoch 8/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 1.9854 - MSE: 9.0951e-06 - val_loss: 0.1838 - val_MSE: 4.2177e-06\n",
      "Epoch 9/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 1.4221 - MSE: 8.2573e-06 - val_loss: 0.1291 - val_MSE: 6.4855e-06\n",
      "Epoch 10/100\n",
      "277/315 [=========================>....] - ETA: 0s - loss: 0.1064 - MSE: 4.7500e-06\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.1036 - MSE: 4.6233e-06 - val_loss: 0.0715 - val_MSE: 3.5077e-06\n",
      "Epoch 11/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0701 - MSE: 3.1601e-06 - val_loss: 0.0656 - val_MSE: 2.9703e-06\n",
      "Epoch 12/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0672 - MSE: 2.9137e-06 - val_loss: 0.0653 - val_MSE: 2.8419e-06\n",
      "Epoch 13/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0668 - MSE: 2.8602e-06 - val_loss: 0.0646 - val_MSE: 2.8450e-06\n",
      "Epoch 14/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0670 - MSE: 2.8213e-06 - val_loss: 0.0675 - val_MSE: 2.9180e-06\n",
      "Epoch 15/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0671 - MSE: 2.7763e-06 - val_loss: 0.0654 - val_MSE: 2.8532e-06\n",
      "Epoch 16/100\n",
      "292/315 [==========================>...] - ETA: 0s - loss: 0.0678 - MSE: 2.7368e-06\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0676 - MSE: 2.7454e-06 - val_loss: 0.0655 - val_MSE: 2.6477e-06\n",
      "Epoch 17/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0650 - MSE: 2.7063e-06 - val_loss: 0.0640 - val_MSE: 2.7441e-06\n",
      "Epoch 18/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0648 - MSE: 2.6912e-06 - val_loss: 0.0656 - val_MSE: 2.8107e-06\n",
      "Epoch 19/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0649 - MSE: 2.6889e-06 - val_loss: 0.0685 - val_MSE: 2.8697e-06\n",
      "Epoch 20/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0648 - MSE: 2.6768e-06 - val_loss: 0.0635 - val_MSE: 2.6566e-06\n",
      "Epoch 21/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0647 - MSE: 2.6662e-06 - val_loss: 0.0632 - val_MSE: 2.6461e-06\n",
      "Epoch 22/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0648 - MSE: 2.6523e-06 - val_loss: 0.0643 - val_MSE: 2.7225e-06\n",
      "Epoch 23/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0647 - MSE: 2.6318e-06 - val_loss: 0.0632 - val_MSE: 2.6556e-06\n",
      "Epoch 24/100\n",
      "274/315 [=========================>....] - ETA: 0s - loss: 0.0644 - MSE: 2.6124e-06\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0651 - MSE: 2.6159e-06 - val_loss: 0.0638 - val_MSE: 2.5574e-06\n",
      "Epoch 25/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0639 - MSE: 2.5992e-06 - val_loss: 0.0633 - val_MSE: 2.5660e-06\n",
      "Epoch 26/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0638 - MSE: 2.5951e-06 - val_loss: 0.0631 - val_MSE: 2.6320e-06\n",
      "Epoch 27/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0638 - MSE: 2.5903e-06 - val_loss: 0.0627 - val_MSE: 2.5983e-06\n",
      "Epoch 28/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0638 - MSE: 2.5868e-06 - val_loss: 0.0628 - val_MSE: 2.6185e-06\n",
      "Epoch 29/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0638 - MSE: 2.5804e-06 - val_loss: 0.0628 - val_MSE: 2.5604e-06\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/315 [=========================>....] - ETA: 0s - loss: 0.0632 - MSE: 2.5717e-06\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0638 - MSE: 2.5730e-06 - val_loss: 0.0630 - val_MSE: 2.5456e-06\n",
      "Epoch 31/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0634 - MSE: 2.5646e-06 - val_loss: 0.0627 - val_MSE: 2.5898e-06\n",
      "Epoch 32/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0634 - MSE: 2.5654e-06 - val_loss: 0.0627 - val_MSE: 2.5525e-06\n",
      "Epoch 33/100\n",
      "275/315 [=========================>....] - ETA: 0s - loss: 0.0635 - MSE: 2.5623e-06\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0634 - MSE: 2.5621e-06 - val_loss: 0.0627 - val_MSE: 2.5547e-06\n",
      "Epoch 34/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5611e-06 - val_loss: 0.0626 - val_MSE: 2.5716e-06\n",
      "Epoch 35/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5612e-06 - val_loss: 0.0626 - val_MSE: 2.5689e-06\n",
      "Epoch 36/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5598e-06 - val_loss: 0.0626 - val_MSE: 2.5748e-06\n",
      "Epoch 37/100\n",
      "275/315 [=========================>....] - ETA: 0s - loss: 0.0627 - MSE: 2.5589e-06\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5600e-06 - val_loss: 0.0626 - val_MSE: 2.5773e-06\n",
      "Epoch 38/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5597e-06 - val_loss: 0.0626 - val_MSE: 2.5651e-06\n",
      "Epoch 39/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5572e-06 - val_loss: 0.0626 - val_MSE: 2.5723e-06\n",
      "Epoch 40/100\n",
      "276/315 [=========================>....] - ETA: 0s - loss: 0.0636 - MSE: 2.5715e-06\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5594e-06 - val_loss: 0.0626 - val_MSE: 2.5637e-06\n",
      "Epoch 41/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5555e-06 - val_loss: 0.0626 - val_MSE: 2.5670e-06\n",
      "Epoch 42/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5573e-06 - val_loss: 0.0626 - val_MSE: 2.5681e-06\n",
      "Epoch 43/100\n",
      "279/315 [=========================>....] - ETA: 0s - loss: 0.0633 - MSE: 2.5314e-06\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5564e-06 - val_loss: 0.0626 - val_MSE: 2.5698e-06\n",
      "Epoch 44/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5579e-06 - val_loss: 0.0626 - val_MSE: 2.5698e-06\n",
      "Epoch 45/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5581e-06 - val_loss: 0.0626 - val_MSE: 2.5696e-06\n",
      "Epoch 46/100\n",
      "275/315 [=========================>....] - ETA: 0s - loss: 0.0635 - MSE: 2.5489e-06\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5578e-06 - val_loss: 0.0626 - val_MSE: 2.5694e-06\n",
      "Epoch 47/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5576e-06 - val_loss: 0.0626 - val_MSE: 2.5694e-06\n",
      "Epoch 48/100\n",
      "315/315 [==============================] - 0s 1ms/step - loss: 0.0633 - MSE: 2.5575e-06 - val_loss: 0.0626 - val_MSE: 2.5694e-06\n",
      "Epoch 00048: early stopping\n",
      "Fold 4 NN: 0.25021\n"
     ]
    }
   ],
   "source": [
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 4\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = ['stock_id','log_return1','log_return2','trade_log_return1']\n",
    "\n",
    "train[pred_name] = 0\n",
    "test['target'] = 0\n",
    "\n",
    "for dev_index, val_index in kf.split(range(len(train))):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    #Bottleneck ? \n",
    "    X_train = train.loc[dev_index, features_to_consider]\n",
    "    y_train = train.loc[dev_index, target_name].values\n",
    "    X_test = train.loc[val_index, features_to_consider]\n",
    "    y_test = train.loc[val_index, target_name].values\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.01),\n",
    "        loss=tf.keras.metrics.mean_squared_error,\n",
    "        metrics=['MSE'],\n",
    "    )\n",
    "\n",
    "\n",
    "    num_data = X_train[['log_return1','log_return2','trade_log_return1']]\n",
    "    cat_data = X_train['stock_id']\n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[['log_return1','log_return2','trade_log_return1']]\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target, \n",
    "              sample_weight = 1/np.square(target),\n",
    "              batch_size=1024,\n",
    "              epochs=100,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test, 1/np.square(y_test)),\n",
    "              callbacks=[es, plateau],\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    test[target_name] += model.predict([test['stock_id'], test[['log_return1','log_return2','trade_log_return1']]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-10T14:47:25.484946Z",
     "iopub.status.busy": "2021-07-10T14:47:25.484546Z",
     "iopub.status.idle": "2021-07-10T14:47:25.508616Z",
     "shell.execute_reply": "2021-07-10T14:47:25.507661Z",
     "shell.execute_reply.started": "2021-07-10T14:47:25.484914Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test[target_name] = test[target_name]/n_folds\n",
    "\n",
    "score = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\n",
    "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test[['row_id', target_name]].head(2))\n",
    "test[['row_id', target_name]].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
